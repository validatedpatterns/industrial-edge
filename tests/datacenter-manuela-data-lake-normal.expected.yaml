---
# Source: central-s3-store/templates/central-s3-store/kafka-to-s3-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-to-s3-config
  namespace: manuela-data-lake
data:
  application.properties: |
    
    kafka.broker.uri=prod-kafka-cluster-kafka-bootstrap.manuela-data-lake.svc:9092
    kafka.broker.topic.temperature=manuela-factory.iot-sensor-sw-temperature
    kafka.broker.topic.vibration=manuela-factory.iot-sensor-sw-vibration

    s3.region=AWSREGION
    s3.bucket.name=BUCKETNAME
    s3.message.aggregation.count=50
    s3.custom.endpoint.enabled=false
    # Convert this directory into a helm chart and use templating to set this
    s3.custom.endpoint.url=s3-openshift-storage.region.example.com
---
# Source: central-s3-store/templates/central-s3-store/kafka-to-s3-integration.yaml
apiVersion: camel.apache.org/v1
kind: Integration
metadata:
  name: kafka-to-s3-integration
  namespace: manuela-data-lake
  annotations:
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true 
spec:
  configuration:
    - type: configmap
      value: kafka-to-s3-config
    - type: secret
      value: s3-secret  
  profile: OpenShift
  sources:
    - content: |
       // dependency=camel:camel-endpointdsl
        package com.redhat.manuela.routes;
        import java.io.ByteArrayInputStream;
        import java.util.Iterator;
        import java.util.List;

        import org.apache.camel.Exchange;
        import org.apache.camel.Processor;
        import org.apache.camel.PropertyInject;
        import org.apache.camel.builder.RouteBuilder;
        import org.apache.camel.component.aws2.s3.AWS2S3Constants;
        import org.apache.camel.builder.endpoint.dsl.AWS2S3EndpointBuilderFactory;
        import org.apache.camel.model.OnCompletionDefinition;
        import org.apache.camel.processor.aggregate.GroupedBodyAggregationStrategy;
        import org.slf4j.Logger;
        import org.slf4j.LoggerFactory;

        public class Kafka2S3Route extends RouteBuilder {

            private static final Logger LOGGER = LoggerFactory.getLogger(Kafka2S3Route.class);

            @PropertyInject("s3.custom.endpoint.enabled")
            private String s3_custom_endpoint_enabled;

            @PropertyInject("s3.custom.endpoint.url")
            private String s3_custom_endpoint_url;

            @PropertyInject("s3.accessKey")
            private String s3_accessKey;
            @PropertyInject("s3.secretKey")
            private String s3_secretKey;
            @PropertyInject("s3.message.aggregation.count")
            private String s3_message_aggregation_count;

            @PropertyInject("s3.region")
            private String s3_region;
            @Override
            public void configure() throws Exception {
                storeTemperatureInS3();
                storeVibrationInS3();
            }

            private void storeVibrationInS3() {
                String key = "accessKey=RAW(" + s3_accessKey + ")";
                String secret = "&secretKey=RAW(" + s3_secretKey + ")";
                String region = "&region=" + s3_region;
                String s3params = key
                + secret
                + region;

                from("kafka:manuela-factory.iot-sensor-sw-vibration?brokers=prod-kafka-cluster-kafka-bootstrap.manuela-data-lake.svc:9092")
                    .convertBodyTo(String.class)
                    .aggregate(simple("true"), new GroupedBodyAggregationStrategy()).completionSize(s3_message_aggregation_count)
                    .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                List<Exchange> data = exchange.getIn().getBody(List.class);
                                StringBuffer sb = new StringBuffer();
                                for (Iterator iterator = data.iterator(); iterator.hasNext();) {
                                    String ex = (String) iterator.next();
                                    sb.append(ex+"\\n");
                                }
                                exchange.getIn().setBody(new ByteArrayInputStream(sb.toString().getBytes()));
                            }
                        })
                    // .to(\"file:/var/tmp/\");
                    .setHeader(AWS2S3Constants.KEY, simple("manuela-prod-vibration-${headers[kafka.KEY]}-${date:now}.txt"))
                    .to("aws2-s3://BUCKETNAME?" + s3params)
                    .log("Uploaded from [ ${headers[kafka.KEY]} ] Vibration dataset to S3");
            }
            private void storeTemperatureInS3() {
                String key = "accessKey=RAW(" + s3_accessKey + ")";
                String secret = "&secretKey=RAW(" + s3_secretKey + ")";
                String region = "&region=" + s3_region;
                String s3params = key
                   + secret
                   + region;
                from("kafka:manuela-factory.iot-sensor-sw-temperature?brokers=prod-kafka-cluster-kafka-bootstrap.manuela-data-lake.svc:9092")
                    .convertBodyTo(String.class)
                    .aggregate(simple("true"), new GroupedBodyAggregationStrategy()).completionSize(s3_message_aggregation_count)
                    .process(new Processor() {
                            @Override
                            public void process(Exchange exchange) throws Exception {
                                List<Exchange> data = exchange.getIn().getBody(List.class);
                                StringBuffer sb = new StringBuffer();
                                for (Iterator iterator = data.iterator(); iterator.hasNext();) {
                                    String ex = (String) iterator.next();
                                    sb.append(ex+"\n");
                                }
                                exchange.getIn().setBody(new ByteArrayInputStream(sb.toString().getBytes()));
                            }
                        })
                    // .to(\"file:/var/tmp/\");
                    .setHeader(AWS2S3Constants.KEY, simple("manuela-prod-temperature-${headers[kafka.KEY]}-${date:now}.txt"))
                    .to("aws2-s3://BUCKETNAME?" + s3params)
                    .log("Uploaded Temperature from [ ${headers[kafka.KEY]} ] dataset to S3");
            }
            @Override
            public OnCompletionDefinition onCompletion() {
                return super.onCompletion();
            }
        }
      name: Kafka2S3Route.java
---
# Source: central-s3-store/templates/central-s3-store/camel-k-integration-platform.yaml
apiVersion: camel.apache.org/v1
kind: IntegrationPlatform
metadata:
  name: camel-k
  labels:
    app: "camel-k"
  annotations:
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true 
spec:
  configuration:
  - type: repository
    value: https://maven.repository.redhat.com/ga/all@id=redhat.ea
---
# Source: central-s3-store/templates/manuela-kafka-cluster/kafka-cluster.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: prod-kafka-cluster
  namespace: manuela-data-lake
  annotations:
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
spec:
  kafka:
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
      - name: external
        port: 9094
        type: route
        tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.min.isr: 2
      transaction.state.log.replication.factor: 3
    storage:
      type: ephemeral
  zookeeper:
    replicas: 3
    storage:
      type: ephemeral  
  entityOperator:
    topicOperator: {}
    userOperator: {}
---
# Source: central-s3-store/templates/factory-data-lake-secret-policy.yaml
apiVersion: policy.open-cluster-management.io/v1
kind: PlacementBinding
metadata:
  name: factory-secret-data-lake-placement-binding
placementRef:
  name: factory-secret-data-lake-placement
  kind: PlacementRule
  apiGroup: apps.open-cluster-management.io
subjects:
  - name: factory-secret-data-lake-policy
    kind: Policy
    apiGroup: policy.open-cluster-management.io
---
# Source: central-s3-store/templates/factory-data-lake-secret-policy.yaml
# We need to run this on any managed cluster but not on the HUB
apiVersion: apps.open-cluster-management.io/v1
kind: PlacementRule
metadata:
  name: factory-secret-data-lake-placement
spec:
  # This will go to all clusters
  clusterConditions:
    - status: 'True'
      type: ManagedClusterConditionAvailable
  clusterSelector:
    matchExpressions:
      - key: local-cluster
        operator: NotIn
        values:
          - 'true'
---
# Source: central-s3-store/templates/factory-data-lake-secret-policy.yaml
apiVersion: policy.open-cluster-management.io/v1
kind: Policy
metadata:
  name: factory-secret-data-lake-policy
spec:
  remediationAction: enforce
  disabled: false
  policy-templates:
    - objectDefinition:
        apiVersion: policy.open-cluster-management.io/v1
        kind: ConfigurationPolicy
        metadata:
          name: factory-secret-data-lake
          annotations:
            apps.open-cluster-management.io/deployables: "secret"
        spec:
          remediationAction: enforce
          severity: med
          namespaceSelector:
            include:
              - default
          object-templates:
            - complianceType: mustonlyhave
              objectDefinition:
                kind: Secret
                type: Opaque
                metadata:
                  name: prod-kafka-cluster-cluster-ca-cert
                  namespace: manuela-stormshift-messaging
                apiVersion: v1
                data:
                  ca.crt: '{{hub index (lookup "v1" "Secret" "manuela-data-lake" "prod-kafka-cluster-cluster-ca-cert").data "ca.crt" hub}}'
